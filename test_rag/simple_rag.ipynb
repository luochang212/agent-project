{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637328f3-5a80-4a9d-9fb8-3b620cfec838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:29:48.274719Z",
     "iopub.status.busy": "2025-05-13T12:29:48.274494Z",
     "iopub.status.idle": "2025-05-13T12:29:48.278097Z",
     "shell.execute_reply": "2025-05-13T12:29:48.277428Z",
     "shell.execute_reply.started": "2025-05-13T12:29:48.274709Z"
    }
   },
   "source": [
    "## 简单的 RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fdde9d-2a95-4aca-a521-0aa82b56bace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:35:54.499094Z",
     "iopub.status.busy": "2025-05-13T13:35:54.498890Z",
     "iopub.status.idle": "2025-05-13T13:35:54.503460Z",
     "shell.execute_reply": "2025-05-13T13:35:54.502644Z",
     "shell.execute_reply.started": "2025-05-13T13:35:54.499077Z"
    }
   },
   "outputs": [],
   "source": [
    "# !uv pip install -qU langchain-openai langchain-chroma langchain-text-splitters langchain-community langgraph langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf40088-6505-4bcd-b00d-e38cf41119cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:35:54.553404Z",
     "iopub.status.busy": "2025-05-13T13:35:54.552458Z",
     "iopub.status.idle": "2025-05-13T13:35:56.366748Z",
     "shell.execute_reply": "2025-05-13T13:35:56.366311Z",
     "shell.execute_reply.started": "2025-05-13T13:35:54.553367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "EMBEDDING_MODEL_PATH = '../model/BAAI/bge-m3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adc8c6-bf4c-4d61-b56b-6239bf322d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:14:40.955855Z",
     "iopub.status.busy": "2025-04-30T17:14:40.955408Z",
     "iopub.status.idle": "2025-04-30T17:14:40.964890Z",
     "shell.execute_reply": "2025-04-30T17:14:40.963301Z",
     "shell.execute_reply.started": "2025-04-30T17:14:40.955832Z"
    }
   },
   "source": [
    "在运行本 notebook 前，请先用 vllm 启动 qwen3 大模型推理服务。\n",
    "\n",
    "**1）qwen3 模型推理服务**\n",
    "\n",
    "大模型推理服务的相关配置:\n",
    "\n",
    "- 虚拟环境部署教程在 `/test_qwen3/qwen3_vllm.ipynb`\n",
    "- 模型下载脚本在 `/model/download_qwen.py`\n",
    "- 启动脚本在 `/test_qwen3/vllm_server.sh`\n",
    "\n",
    "下载模型并配置好虚拟环境后，来到项目根目录，打开 `test_qwen3` 文件夹，运行服务端启动脚本：\n",
    "\n",
    "```bash\n",
    "cd test_qwen3\n",
    "bash vllm_server.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca4b69-7411-4359-b65e-c240c78447ec",
   "metadata": {},
   "source": [
    "对于遵循 OpenAI 接口格式的本地模型服务，使用 `ChatOpenAI` 来连接。参考文档：[api.python.langchain.com](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fa03aa-c542-42a6-9140-9b03a3f92639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:01.233753Z",
     "iopub.status.busy": "2025-05-13T13:36:01.233198Z",
     "iopub.status.idle": "2025-05-13T13:36:01.280213Z",
     "shell.execute_reply": "2025-05-13T13:36:01.279679Z",
     "shell.execute_reply.started": "2025-05-13T13:36:01.233730Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name='Qwen3-0.6B-FP8',\n",
    "    openai_api_base='http://localhost:8000/v1',\n",
    "    openai_api_key='token-kcgyrk'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f744e73-594f-4be6-bc30-8dee6a94f56b",
   "metadata": {},
   "source": [
    "**2）bge-m3 文本 embedding 推理服务**\n",
    "\n",
    "使用 [BAAI/bge-m3](https://www.modelscope.cn/models/BAAI/bge-m3) 作为文本 embedding 模型。\n",
    "\n",
    "可以使用 Ollama 部署 bge-m3 文本 embedding 推理服务。安装和使用 [Ollama](https://ollama.com/) 的详细方法，参阅我的博客 [《本地部署大模型：Ollama 和 vLLM》](https://luochang212.github.io/posts/llm_deploy/)，这里就不详述了。\n",
    "\n",
    "在 Windows 系统下，使用以下代码下载模型并启动推理服务：\n",
    "\n",
    "```bash\n",
    "# 查看 Ollama 版本\n",
    "ollama --version\n",
    "\n",
    "# 下载 bge-m3 模型\n",
    "ollama pull bge-m3\n",
    "\n",
    "# Windows Powershell 下，启动 Ollama 服务，指定端口为 11435\n",
    "$env:OLLAMA_HOST=\"http://0.0.0.0:11435\"; ollama serve\n",
    "```\n",
    "\n",
    "由于我的 `jupyter lab` 是在 Windows 的 wsl 的 Ubuntu 系统中运行的，因此我还需要获取 Windows 系统主机的 ip，以访问运行于主机上的 Ollama 服务。\n",
    "\n",
    "在 Powershell 执行 `ipconfig` 命令，获取 IP 配置列表：\n",
    "\n",
    "```\n",
    "无线局域网适配器 WLAN:\n",
    "\n",
    "   IPv4 地址 . . . . . . . . . . . . : 192.168.16.225\n",
    "\n",
    "```\n",
    "\n",
    "我的主机的 Ipv4 地址为 `192.168.16.225`，因此我的 `base_url=\"http://192.168.16.225:11435\"`，你需要在这里替换成你的 IP。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c39809c-09b6-41ac-b58d-08e21910e332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:06.175492Z",
     "iopub.status.busy": "2025-05-13T13:36:06.174859Z",
     "iopub.status.idle": "2025-05-13T13:36:06.180131Z",
     "shell.execute_reply": "2025-05-13T13:36:06.179081Z",
     "shell.execute_reply.started": "2025-05-13T13:36:06.175464Z"
    }
   },
   "outputs": [],
   "source": [
    "base_url=\"http://192.168.16.225:11435\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd08aec0-4ef0-48ab-9b07-a0eceed30dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:06.960327Z",
     "iopub.status.busy": "2025-05-13T13:36:06.959618Z",
     "iopub.status.idle": "2025-05-13T13:36:06.970221Z",
     "shell.execute_reply": "2025-05-13T13:36:06.969026Z",
     "shell.execute_reply.started": "2025-05-13T13:36:06.960293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.status_code: 200\n",
      "response.text: Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# 使用 request 确认可以访问 Ollama 服务\n",
    "response = requests.get(base_url, timeout=5)\n",
    "\n",
    "print(f'response.status_code: {response.status_code}')\n",
    "print(f'response.text: {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d60b177-2268-4bc3-9e73-6affb3e21eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:08.165733Z",
     "iopub.status.busy": "2025-05-13T13:36:08.165567Z",
     "iopub.status.idle": "2025-05-13T13:36:12.139173Z",
     "shell.execute_reply": "2025-05-13T13:36:12.138658Z",
     "shell.execute_reply.started": "2025-05-13T13:36:08.165723Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"这是第一个示例文本\",\n",
    "    \"这是第二个完全不同的文本\",\n",
    "    \"这是第三个与第一个相似的文本\"\n",
    "]\n",
    "\n",
    "# 使用 embed_documents 获取多个嵌入向量\n",
    "embedding_vectors = embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80906ef-9aad-40d1-b3e8-ac40a2b963a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:12.139981Z",
     "iopub.status.busy": "2025-05-13T13:36:12.139762Z",
     "iopub.status.idle": "2025-05-13T13:36:12.143140Z",
     "shell.execute_reply": "2025-05-13T13:36:12.142581Z",
     "shell.execute_reply.started": "2025-05-13T13:36:12.139970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(embedding_vectors): 3\n",
      "type(embedding_vectors): <class 'list'>\n",
      "np.array(embedding_vectors).shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(f'len(embedding_vectors): {len(embedding_vectors)}')\n",
    "print(f'type(embedding_vectors): {type(embedding_vectors)}')\n",
    "print(f'np.array(embedding_vectors).shape: {np.array(embedding_vectors).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d156f0-cd01-4b85-9dbe-98e534a77042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:13.105422Z",
     "iopub.status.busy": "2025-05-13T13:36:13.104870Z",
     "iopub.status.idle": "2025-05-13T13:36:13.257275Z",
     "shell.execute_reply": "2025-05-13T13:36:13.256871Z",
     "shell.execute_reply.started": "2025-05-13T13:36:13.105399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 embed_query 获取单个查询嵌入\n",
    "query = \"这是一个测试文本\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac45e8-334e-43f5-99f8-235e53466be2",
   "metadata": {},
   "source": [
    "**3）使用向量数据库 Chroma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a54b132-b0c8-474a-ba00-d9420079ab07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:16.244055Z",
     "iopub.status.busy": "2025-05-13T13:36:16.243811Z",
     "iopub.status.idle": "2025-05-13T13:36:16.432167Z",
     "shell.execute_reply": "2025-05-13T13:36:16.431507Z",
     "shell.execute_reply.started": "2025-05-13T13:36:16.244041Z"
    }
   },
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"rag_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0293d28f-5fdc-49bc-9f64-9eeddcdee13b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:17.268531Z",
     "iopub.status.busy": "2025-05-13T13:36:17.267535Z",
     "iopub.status.idle": "2025-05-13T13:36:21.233852Z",
     "shell.execute_reply": "2025-05-13T13:36:21.232652Z",
     "shell.execute_reply.started": "2025-05-13T13:36:17.268500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://luochang212.github.io/posts/docker_command/\",)\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f9358f-439a-46ad-b39e-0b42f93a8110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:21.905582Z",
     "iopub.status.busy": "2025-05-13T13:36:21.903935Z",
     "iopub.status.idle": "2025-05-13T13:36:21.909568Z",
     "shell.execute_reply": "2025-05-13T13:36:21.908865Z",
     "shell.execute_reply.started": "2025-05-13T13:36:21.905551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source', 'title', 'language'])\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b84f9c9-2df4-465d-8cc8-1176039ccb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:22.776628Z",
     "iopub.status.busy": "2025-05-13T13:36:22.775981Z",
     "iopub.status.idle": "2025-05-13T13:36:22.780351Z",
     "shell.execute_reply": "2025-05-13T13:36:22.779644Z",
     "shell.execute_reply.started": "2025-05-13T13:36:22.776609Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a5f643-054e-4590-94e9-6f3f4191ca76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:23.426581Z",
     "iopub.status.busy": "2025-05-13T13:36:23.425504Z",
     "iopub.status.idle": "2025-05-13T13:36:23.434175Z",
     "shell.execute_reply": "2025-05-13T13:36:23.433152Z",
     "shell.execute_reply.started": "2025-05-13T13:36:23.426552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89540c9-0b9b-47ee-91a3-c6ea5425d131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:23.963990Z",
     "iopub.status.busy": "2025-05-13T13:36:23.963439Z",
     "iopub.status.idle": "2025-05-13T13:36:23.968888Z",
     "shell.execute_reply": "2025-05-13T13:36:23.967880Z",
     "shell.execute_reply.started": "2025-05-13T13:36:23.963961Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98e1604-6924-4433-9261-3a145b0a1287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:24.470719Z",
     "iopub.status.busy": "2025-05-13T13:36:24.470364Z",
     "iopub.status.idle": "2025-05-13T13:36:25.779605Z",
     "shell.execute_reply": "2025-05-13T13:36:25.778705Z",
     "shell.execute_reply.started": "2025-05-13T13:36:24.470696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd683b9e-7bf9-448e-8a6b-c49c46cec33d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:25.781963Z",
     "iopub.status.busy": "2025-05-13T13:36:25.780751Z",
     "iopub.status.idle": "2025-05-13T13:36:27.070145Z",
     "shell.execute_reply": "2025-05-13T13:36:27.068136Z",
     "shell.execute_reply.started": "2025-05-13T13:36:25.781932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canva/miniconda3/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6575b7f4-70db-4d23-af29-784c2cdfc579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:27.524279Z",
     "iopub.status.busy": "2025-05-13T13:36:27.523612Z",
     "iopub.status.idle": "2025-05-13T13:36:27.534585Z",
     "shell.execute_reply": "2025-05-13T13:36:27.533387Z",
     "shell.execute_reply.started": "2025-05-13T13:36:27.524254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c803eb97-5136-4dd6-873b-af9e1f1680dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:38:36.036581Z",
     "iopub.status.busy": "2025-05-13T13:38:36.035667Z",
     "iopub.status.idle": "2025-05-13T13:38:40.387802Z",
     "shell.execute_reply": "2025-05-13T13:38:40.387205Z",
     "shell.execute_reply.started": "2025-05-13T13:38:36.036546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好的，用户的问题是如何启动容器。我需要从提供的上下文中找到相关的信息。首先，看到的Context中提到了在命令行中运行容器的步骤，包括使用docker run来启动。然后，还有两个具体的例子，分别是测试容器和运行容器。比如，测试容器使用了--rm、--gpus和--name等参数，而运行容器使用了-d、-p、-v等选项。\n",
      "\n",
      "用户的问题只需要启动容器的方法，所以应该总结这两个例子中的关键步骤。要注意使用三个句子，确保简洁。同时，要确保答案准确，不偏离提供的上下文内容。检查是否有足够的信息，如果有的话，就直接引用。如果没有的话，可能需要判断是否还需要补充，但根据上下文，答案应该是两个命令的例子，所以用简洁的方式表达即可。\n",
      "</think>\n",
      "\n",
      "启动容器可通过以下步骤实现：  \n",
      "1. 使用 `docker run` 命令指定镜像并运行容器，例如 `docker run --gpus all --name jupyter_server_app jupyter_server_image /bin/bash`。  \n",
      "2. 或使用 `docker run` 的 `-d` 参数以后台模式运行，例如 `docker run -d -p 9999:8888 jupyter_server_image`，并结合端口映射和挂载路径进行配置。\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"如何启动容器\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b340b2-f4b5-44e1-b971-74964dd7803a",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [Build a Retrieval Augmented Generation (RAG) App: Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15789172-7b76-495b-b2c7-a2b873a1242e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
